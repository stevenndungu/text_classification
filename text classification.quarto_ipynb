{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"NLP Text Classification Summary Report\"\n",
        "date: today\n",
        "date-format: long\n",
        "author: \"Steven  Ndung'u\"\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-depth: 2\n",
        "    toc-location: left\n",
        "    page-layout: full\n",
        "    theme:\n",
        "          light: flatly\n",
        "          dark: darkly\n",
        "    number-sections: false\n",
        "    highlighting: true\n",
        "    smooth-scroll: true\n",
        "    code-fold: true\n",
        "    highlighting-style: GitHub\n",
        "    self-contained: true\n",
        "execute:\n",
        "    echo: true\n",
        "    warning: false\n",
        "    enable: true\n",
        "\n",
        "title-block-banner: true\n",
        "\n",
        "---\n",
        "\n",
        "```{=html}\n",
        "<style type=\"text/css\">\n",
        "\n",
        "h1.title {\n",
        "  font-size: 0px;\n",
        "  color: White;\n",
        "  text-align: center;\n",
        "}\n",
        "h4.author { /* Header 4 - and the author and data headers use this too  */\n",
        "    font-size: 16px;\n",
        "  font-family: \"Source Sans Pro Semibold\", Times, serif;\n",
        "  color: Red;\n",
        "  text-align: center;\n",
        "}\n",
        "h4.date { /* Header 4 - and the author and data headers use this too  */\n",
        "  font-size: 16px;\n",
        "  font-family: \"Source Sans Pro Semibold\", Times, serif;\n",
        "  color: Red;\n",
        "  text-align: center;\n",
        "}\n",
        "</style>\n",
        "```\n",
        "\n",
        "\n",
        "------------------------------------------------------------------------\n",
        ":::{.column-page}\n",
        "\n",
        "::: {style=\"text-align:center\"}\n",
        "<h2>Text Classification Challenge</h2>\n",
        ":::\n",
        "\n",
        "</br>\n"
      ],
      "id": "01887f32"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| code-fold: false\n",
        "#| \n",
        "###################################################\n",
        "\n",
        "###################################################\n",
        "#$Env:QUARTO_PYTHON = \"C:\\Users\\P307791\\Anaconda3\\python.exe\"\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = 'python'\n",
        "from scipy import stats\n",
        "\n",
        "from IPython.display import display, Markdown, HTML\n",
        "from itables import init_notebook_mode\n",
        "init_notebook_mode(all_interactive=True)\n",
        "from itables import show\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "#pio.renderers.default = \"notebook\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from scipy import stats\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "daf3cd7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem Statement\n",
        "\n",
        "The task is to build a text classification model that accurately predicts whether a given movie review expresses a positive or negative sentiment. Sentiment analysis is a critical task in NLP with applications in marketing, customer feedback, social media monitoring, and more. Accurately classifying sentiments can provide valuable insights into customer opinions and help businesses make data-driven decisions.\n",
        "\n",
        "### Why This Task is Important\n",
        "\n",
        "Understanding customer sentiment through text data is crucial for businesses and organizations to respond effectively to customer needs and preferences. By automating the sentiment analysis process, companies can efficiently analyze vast amounts of data, identify trends, and make informed strategic decisions. For this challenge, we will use the IMDb dataset, a widely-used benchmark in sentiment analysis, to train and evaluate our model.\n",
        "\n",
        "### Dataset Description\n",
        "\n",
        "The dataset used for this challenge is the IMDb movie reviews dataset, which contains 50,000 reviews labeled as either positive or negative. This dataset is balanced, with an equal number of positive and negative reviews, making it ideal for training and evaluating sentiment analysis models.\n",
        "\n",
        "- **Columns:**\n",
        "  - `review`: The text of the movie review.\n",
        "  - `sentiment`: The sentiment label (`positive` or `negative`).\n",
        "\n",
        "The IMDb dataset provides a real-world scenario where understanding sentiment can offer insights into public opinion about movies, directors, and actors, as well as broader trends in the entertainment industry.\n",
        "\n",
        "### Approach\n",
        "\n",
        "Transformers have revolutionized NLP by allowing models to consider the context of a word based on surrounding words, enabling better understanding and performance on various tasks, including sentiment analysis. Their ability to transfer learning from massive datasets and adapt to specific tasks makes them highly effective for text classification.\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "</br>\n",
        "\n",
        "### Data Exploration and Preprocessing\n",
        "\n",
        "The data is generally clean the only preprocessing required is to remove any special characters and convert the text to lowercase.\n",
        "\n",
        "#### Preview the data:"
      ],
      "id": "c35017ae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| code-fold: false\n",
        "#| \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, os, random, torch\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from IPython.display import display, Markdown, HTML\n",
        "from utils import *\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "ebd4b7c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_csv('IMDB Dataset.csv')\n",
        "df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x=='positive' else 0)\n",
        "df['review'] = df['review'].apply(clean_text)"
      ],
      "id": "9493b10d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| code-fold: false\n",
        "#|\n",
        "res = df.head()\n",
        "\n",
        "html_table = res.to_html(index=True)\n",
        "\n",
        "# Wrap in a scrollable div\n",
        "scrollable_table = f\"\"\"\n",
        "<div style=\"height: 400px; width: 100%; overflow-x: auto; overflow-y: auto;\">\n",
        "    {html_table}\n",
        "</div>\n",
        "\"\"\"\n",
        "# Display the scrollable table\n",
        "display(HTML(scrollable_table))"
      ],
      "id": "5ad99e9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Implementation\n"
      ],
      "id": "81c3b91d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from utils import *\n",
        "\n",
        "\n",
        "# Load IMDb Dataset\n",
        "df = pd.read_csv('IMDB Dataset.csv')\n",
        "#print('IMDB Dataset.csv data loaded ...')\n",
        "# Preprocess the dataset\n",
        "df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)  # Convert sentiments to binary\n",
        "df['review'] = df['review'].apply(clean_text)\n",
        "\n",
        "# Simple tokenization process\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and numbers\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "# Build Vocabulary\n",
        "def build_vocab(reviews):\n",
        "    vocab = Counter()\n",
        "    for review in reviews:\n",
        "        tokens = tokenize(review)\n",
        "        vocab.update(tokens)\n",
        "    \n",
        "    vocab = {word: i+3 for i, (word, _) in enumerate(vocab.most_common())}  # Start index from 3\n",
        "    vocab['<PAD>'] = 0  # Padding token\n",
        "    vocab['<UNK>'] = 1  # Unknown token\n",
        "    vocab['[CLS]'] = 2  # [CLS] token\n",
        "    return vocab\n",
        "\n",
        "# Convert text to numerical indices\n",
        "def text_to_indices(text, vocab, max_len):\n",
        "    tokens = ['[CLS]'] + tokenize(text)\n",
        "    indices = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
        "    if len(indices) < max_len:\n",
        "        indices += [vocab['<PAD>']] * (max_len - len(indices))  # Padding\n",
        "    else:\n",
        "        indices = indices[:max_len]  # Truncate if longer than max_len\n",
        "    return indices\n",
        "\n",
        "# Maximum sequence length (adjusted to include [CLS] token)\n",
        "max_len = 401  # max_len (400) + 1 for [CLS] token\n",
        "\n",
        "# Tokenize and encode the dataset\n",
        "if os.path.exists('tokenized_reviews_vocab.npy') and os.path.exists('vocab.pkl'):\n",
        "    tokenized_reviews = np.load('tokenized_reviews_vocab.npy')\n",
        "    with open('vocab.pkl', 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "else:\n",
        "    reviews = df['review'].tolist()\n",
        "    vocab = build_vocab(reviews)\n",
        "    # Convert reviews to input IDs\n",
        "    tokenized_reviews = [text_to_indices(review, vocab, max_len) for review in reviews]\n",
        "    np.save('tokenized_reviews_vocab.npy', np.array(tokenized_reviews))\n",
        "    with open('vocab.pkl', 'wb') as f:\n",
        "        pickle.dump(vocab, f)\n",
        "\n",
        "# Convert arrays into PyTorch tensors\n",
        "inputs_input_ids = torch.tensor(tokenized_reviews).to(device)\n",
        "labels = torch.tensor(df['sentiment'].values).to(device)\n",
        "\n",
        "#print('Tokenization and vocabulary buidling complete ...')\n",
        "\n",
        "# Split the dataset into training, validation, and test sets (70%, 15%, 15%)\n",
        "train_inputs, valid_test_inputs, train_labels, valid_test_labels = train_test_split(\n",
        "    inputs_input_ids, labels, test_size=0.3, random_state=100, shuffle=True\n",
        ")\n",
        "valid_inputs, test_inputs, valid_labels, test_labels = train_test_split(\n",
        "    valid_test_inputs, valid_test_labels, test_size=0.5, random_state=100, shuffle=True\n",
        ")\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "valid_dataset = TensorDataset(valid_inputs, valid_labels)\n",
        "test_dataset = TensorDataset(test_inputs, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "#print('Data loaders complete ...')"
      ],
      "id": "ddfe8fd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Architecture Overview:\n",
        "@fig-methodology illustrates the proposed workflow model (Transformer-based architecture) designed for IMDb sentiment classification:\n",
        "\n",
        "\n",
        "\n",
        "::: {.column-page layout-ncol=1}\n",
        "\n",
        "![](text_classification.png){#fig-methodology}\n",
        "\n",
        "\n",
        "Proposed Transformer-based architecture for IMDb sentiment classification.\n",
        ":::\n",
        "\n",
        "The transformer-based sentiment analysis model is a sophisticated deep learning architecture designed to effectively capture and classify the sentiment expressed in text data. It leverages the power of transformer encoder blocks, which employ multi-head self-attention mechanisms to understand complex relationships between words, regardless of their distance in the sentence. The model initially processes the input text by converting it into numerical representations, known as token IDs. These tokens are then embedded into a continuous vector space where semantic meaning can be captured. To preserve the context of word order, positional embeddings are added to the token embeddings.\n",
        "\n",
        "The core of the model consists of multiple transformer encoder layers, each containing two main sublayers: multi-head self-attention and position-wise feed-forward neural network. The multi-head self-attention sublayer allows the model to weigh the importance of different words based on their relevance to the overall sentiment. The feed-forward neural network applies non-linear transformations to capture complex patterns in the input data.\n",
        "\n",
        "After passing through the transformer encoder stack, the model selects the hidden state corresponding to the [CLS] token, which serves as a summary representation of the entire sequence. This representation is then fed into a linear classification layer that maps it to a vector of class probabilities. The final output of the model is a prediction of the sentiment expressed in the input text, indicating whether it is positive, negative, or neutral.\n",
        "\n",
        "Why This Architecture is Ideal for Text Classification (IMDb Dataset):\n",
        "\n",
        "The Transformer-based architecture is ideal for IMDb sentiment classification due to its ability to handle variable-length sequences, efficiently capturing long-range dependencies across the text. The self-attention mechanism allows the model to focus on sentiment-bearing words and understand their contextual meaning by attending to all parts of the sequence. The use of the [CLS] token for sequence-level classification encodes the overall sentiment of the entire review, making it effective for binary sentiment prediction. Layer normalization and residual connections stabilize the training of deep architectures, while feed-forward networks introduce non-linearity, helping the model learn complex patterns in the data. Multi-head attention provides diverse perspectives on the input, enhancing the richness of the representations, and positional embeddings maintain the sequence structure. Regularization techniques, such as dropout, prevent overfitting, and the flexibility of hyperparameters allows the model to adapt to different datasets and resources. This architecture is also scalable and aligns with popular models like BERT, offering potential for future transfer learning. \n",
        "\n",
        "### Training and Evaluation\n",
        "\n",
        "To determine the best hyperparameter set, a grid search is performed across various combinations of hyperparameters. For each combination, the model is trained on the training dataset, and validation accuracy is computed by evaluating the model on a separate validation set. This process is repeated for all combinations in the hyperparameter search space, and the corresponding validation accuracies are recorded. The hyperparameter combination that yields the highest validation accuracy is selected as the best or optimal set. Once the optimal hyperparameters are identified, the model is retrained using the training data and validated hyperparameters. Finally, the model's performance is tested on an unseen test dataset to obtain the final test accuracy, which serves as the ultimate measure of the model's effectiveness. This ensures that the model generalizes well and avoids overfitting to the training and validation data.\n"
      ],
      "id": "01fe090a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training function\n",
        "\n",
        "def train(model, train_loader, valid_loader, optimizer, scheduler, loss_fn, device, epochs, early_stopping_patience=5):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "    \n",
        "    best_accuracy = 0  # Track the best validation accuracy\n",
        "    patience_counter = 0  # Counter for early stopping\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100 * correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        \n",
        "        # Evaluate on the validation set\n",
        "        val_loss, val_acc = evaluate(model, valid_loader, loss_fn, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        # Early stopping\n",
        "        if val_acc > best_accuracy:\n",
        "            best_accuracy = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            \n",
        "            print(f\"Epoch {epoch+1}: Best model saved with accuracy: {best_accuracy:.2f}%\")\n",
        "        # else:\n",
        "        #     patience_counter += 1\n",
        "        #     if patience_counter >= early_stopping_patience:\n",
        "        #         print(\"Early stopping triggered.\")\n",
        "        #         break\n",
        "        \n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "    \n",
        "        \n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def hyperparameter_search(hyperparameter_combinations, results_df):\n",
        "    for i, params in enumerate(hyperparameter_combinations):\n",
        "        print(f\"\\nRunning hyperparameter set {i+1}/{len(hyperparameter_combinations)}\")\n",
        "        \n",
        "        # Unpack hyperparameters\n",
        "        lr, num_hidden_layers, num_attention_heads, hidden_size, intermediate_size, hidden_dropout_prob, activation_function = params\n",
        "        \n",
        "        # Prepare DataLoader with current batch_size\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "        valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
        "        \n",
        "        # Initialize the model with current hyperparameters\n",
        "        config = TransformerConfig(\n",
        "            vocab_size=len(vocab),\n",
        "            hidden_size=hidden_size,\n",
        "            num_attention_heads=num_attention_heads,\n",
        "            num_hidden_layers=num_hidden_layers,\n",
        "            intermediate_size=intermediate_size,\n",
        "            hidden_dropout_prob=hidden_dropout_prob,\n",
        "            max_position_embeddings=max_len,\n",
        "            num_labels=2,\n",
        "            activation_function=activation_function\n",
        "        )\n",
        "        model = TransformerForSequenceClassification(config).to(device)\n",
        "        \n",
        "        # Initialize optimizer and loss function\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        \n",
        "        # Train the model\n",
        "        epochs = 100 \n",
        "        \n",
        "        train_losses, val_losses, train_accuracies, val_accuracies = train(model, train_loader, valid_loader, optimizer, scheduler, loss_fn, device, epochs, early_stopping_patience=5)\n",
        "        \n",
        "        # Get the best validation accuracy\n",
        "        best_val_accuracy = max(val_accuracies)\n",
        "        best_val_loss = min(val_losses)\n",
        "        \n",
        "        # Save the results\n",
        "        current_result = pd.DataFrame([{\n",
        "            'lr': lr,\n",
        "            'num_hidden_layers': num_hidden_layers,\n",
        "            'num_attention_heads': num_attention_heads,\n",
        "            'hidden_size': hidden_size,\n",
        "            'intermediate_size': intermediate_size,\n",
        "            'hidden_dropout_prob': hidden_dropout_prob,\n",
        "            'activation_function': activation_function,\n",
        "            'val_loss': best_val_loss,\n",
        "            'val_accuracy': best_val_accuracy\n",
        "         }])\n",
        "\n",
        "        results_df = pd.concat([results_df, current_result], ignore_index=True)\n",
        "        \n",
        "        # save the model if it's the best so far\n",
        "        if best_val_accuracy == results_df['val_accuracy'].max():\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            results_df.to_csv('results_df.csv', index=False)\n",
        "            print(f\"New best model saved with validation accuracy: {best_val_accuracy:.2f}%\")\n",
        "\n",
        "            # Plot loss and accuracy curves\n",
        "            plot_curves(train_losses, val_losses, train_accuracies, val_accuracies)\n",
        "            # Load the best model and evaluate on the test set\n",
        "            model.load_state_dict(torch.load('best_model.pth'))\n",
        "            test_loss, test_acc = evaluate(model, test_loader, loss_fn, device)\n",
        "            print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "        \n",
        "    return results_df\n",
        "\n",
        "# DataFrame to store hyperparameters and validation scores:\n",
        "results_df = pd.DataFrame(columns=[\n",
        "    'lr',\n",
        "    'num_hidden_layers',\n",
        "    'num_attention_heads',\n",
        "    'hidden_size',\n",
        "    'intermediate_size',\n",
        "    'hidden_dropout_prob',\n",
        "    'activation_function',\n",
        "    'val_loss',\n",
        "    'val_accuracy'\n",
        "])\n",
        "\n",
        "#results_df = hyperparameter_search(sampled_combinations, results_df)\n",
        "# Sort the results by validation accuracy\n",
        "#sorted_results = results_df.sort_values(by='val_accuracy', ascending=False)\n",
        "\n",
        "# Display the top 5 configurations\n",
        "#print(\"Top 5 Hyperparameter Configurations:\")\n",
        "#print(sorted_results.head(5))\n",
        "\n",
        "columns = [\n",
        "    'lr',\n",
        "    'num_hidden_layers',\n",
        "    'num_attention_heads',\n",
        "    'hidden_size',\n",
        "    'intermediate_size',\n",
        "    'hidden_dropout_prob',\n",
        "    'activation_function'\n",
        "]\n",
        "\n",
        "df_hyperparams = pd.DataFrame(sampled_combinations, columns=columns)"
      ],
      "id": "bed0fd41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sampled hyperparameters:\n",
        "\n",
        "\n",
        "::: {.callout-important}\n",
        "The selected hyperparameters do not represent all the possible hyperparameters that could be considered in this case. This process can be extended to include adjustments such as changing the activation functions, experimenting with different loss functions, or tuning additional parameters like learning rate schedules or optimizer types. By exploring a broader range of hyperparameters, it’s possible to further optimize the model’s performance and identify configurations that might yield better results.\n",
        ":::\n"
      ],
      "id": "9d4d0547"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| code-fold: false\n",
        "#|\n",
        "\n",
        "\n",
        "html_table = df_hyperparams.to_html(index=True)\n",
        "\n",
        "# Wrap in a scrollable div\n",
        "scrollable_table = f\"\"\"\n",
        "<div style=\"height: 400px; width: 100%; overflow-x: auto; overflow-y: auto;\">\n",
        "    {html_table}\n",
        "</div>\n",
        "\"\"\"\n",
        "# Display the scrollable table\n",
        "display(HTML(scrollable_table))"
      ],
      "id": "e1b86ca2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prediction and Inference\n",
        "\n",
        "Based on the limited grid search best test accuracy of 88% \n",
        "\n",
        "::: {.callout-note}\n",
        "\n",
        "Based on the limited grid search I obtained a best test accuracy of 88%.\n",
        ":::\n"
      ],
      "id": "9920824f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "# test_loss, test_acc = evaluate(model, test_loader, loss_fn, device)\n",
        "\n",
        "# Initialize the model and load the saved state\n",
        "config = TransformerConfig(\n",
        "          vocab_size=len(vocab),\n",
        "          hidden_size=256,\n",
        "          num_attention_heads=8,\n",
        "          num_hidden_layers=8,\n",
        "          intermediate_size=512,\n",
        "          hidden_dropout_prob=0.1,\n",
        "          max_position_embeddings=max_len,\n",
        "          num_labels=2,\n",
        "          activation_function='gelu'\n",
        "      )\n",
        "model = TransformerForSequenceClassification(config).to(device)\n",
        "model.load_state_dict(torch.load('best_model.pth', map_location=torch.device('cpu')))\n",
        "model = model.eval()\n",
        "\n",
        "\n",
        "\n",
        "# - vocab: the vocabulary used for tokenizing text\n",
        "# - max_len: the maximum length of the tokenized review\n",
        "# - class_names: list of class names (e.g., ['negative', 'positive'])\n",
        "# - model: the trained Transformer model\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize and convert the review to input IDs\n",
        "def preprocess_review(review_text, vocab,max_len):\n",
        "\n",
        "\n",
        "\n",
        "  # Tokenize the review text\n",
        "  tokens = review_text.lower()\n",
        "  tokens = re.sub(r'[^a-zA-Z\\s]', '', tokens).split()  # Basic tokenization\n",
        "  # Convert tokens to IDs using the vocabulary\n",
        "  token_ids = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
        "  # Pad or truncate to max_len\n",
        "  if len(token_ids) < max_len:\n",
        "      token_ids += [vocab['<PAD>']] * (max_len - len(token_ids))  # Padding\n",
        "  else:\n",
        "      token_ids = token_ids[:max_len] \n",
        "  return torch.tensor([token_ids]).to(device)  \n",
        "\n",
        "# Predict the sentiment of a review\n",
        "def predict_sentiment(review_text):\n",
        "    # Preprocess the review\n",
        "    input_ids = preprocess_review(review_text, vocab, max_len)\n",
        "    \n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids)\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        _, prediction = torch.max(probabilities, dim=1)\n",
        "    \n",
        "    return int(prediction), probabilities\n",
        "\n",
        "class_names = ['Negative', 'Positive']\n",
        "# Print results for a single review\n",
        "def Print_Results(review_text_row):\n",
        "    selected_review = df.review.iloc[review_text_row]  # Get the review text from the DataFrame\n",
        "    prediction, probabilities = predict_sentiment(selected_review)\n",
        "    sentiment = class_names[prediction]\n",
        "    \n",
        "    # Display the prediction result\n",
        "    print(f\"Selected Review: {selected_review}\")\n",
        "    print(f\"Predicted Sentiment: {sentiment}\")\n",
        "    print(f\"Confidence: {probabilities[0][prediction].item() * 100:.2f}%\")\n",
        "\n",
        "# Example usage with reviews from the dataset:\n",
        "print('#######################################')\n",
        "print('##          Example 1                ##')\n",
        "print('#######################################')\n",
        "print('\\n')\n",
        "Print_Results(2)\n",
        "print('\\n')\n",
        "print('#######################################')\n",
        "print('##          Example 2                ##')\n",
        "print('#######################################')\n",
        "print('\\n')\n",
        "Print_Results(480)"
      ],
      "id": "5059fede",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example of model training and validation curves:\n",
        "\n",
        "[](Train_valid_curves_v4.png)\n",
        "\n",
        "\n",
        "\n",
        "### Model Deployment\n",
        "\n",
        "The model is deployed at: [deployed text classification model](https://textclassificationdemo.streamlit.app/).\n",
        "\n",
        "@fig-negative and @fig-positive provides two examples extracted from the [app](https://textclassificationdemo.streamlit.app/).\n",
        "\n",
        "::: {.column-page layout-ncol=1}\n",
        "\n",
        "![Negative](example1.png){#fig-negative}\n",
        "\n",
        "![Positive](example2.png){#fig-positive}\n",
        "\n",
        "Model predictions for two example reviews from the IMDb dataset.\n",
        ":::\n",
        "\n",
        "More details can be found at my [Github repository](https://github.com/stevenndungu/text_classification):\n",
        "\n",
        " - [Flask App script](https://github.com/stevenndungu/text_classification/blob/main/flask_app.py)\n",
        "\n",
        " - [Streamlit App script](https://github.com/stevenndungu/text_classification/blob/main/streamlit_app.py)\n",
        "\n",
        "\n",
        ":::\n"
      ],
      "id": "15b7d34e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\P307791\\Anaconda3\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}